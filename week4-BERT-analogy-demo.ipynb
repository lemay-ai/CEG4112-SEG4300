{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities:\n",
      "   man: 0.9218\n",
      " woman: 0.9708\n",
      "  king: 0.9723\n",
      " queen: 0.9855\n"
     ]
    }
   ],
   "source": [
    "# BERT analogy demo: (king - man + woman) â‰ˆ queen\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pretrained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Function to get embedding for a word (with neutral context), bc BERT is contextual, it was trained on full sentences, not isolated words\n",
    "def get_embedding(word, template=\"This is a {}.\"):\n",
    "    \"\"\"\n",
    "    Get a contextual BERT embedding for a single word by placing it\n",
    "    inside a neutral sentence template.\n",
    "    Uses the [CLS] token vector as the sentence representation.\n",
    "    \"\"\"\n",
    "    text = template.format(word)\n",
    "    inputs = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding (first token)\n",
    "    vec = outputs.last_hidden_state[0, 0, :]      # (batch_size, seq_len, hidden)\n",
    "    # Normalize to unit length (important for cosine similarity)\n",
    "    vec = F.normalize(vec, p=2, dim=0)            # L2 norm\n",
    "    return vec\n",
    "\n",
    "\n",
    "# Retrieve embeddings\n",
    "words = [\"man\", \"woman\", \"king\", \"queen\"]\n",
    "embeddings = {w: get_embedding(w) for w in words}\n",
    "\n",
    "# Compute analogy: king - man + woman\n",
    "analogy_vector = embeddings[\"king\"] - embeddings[\"man\"] + embeddings[\"woman\"]\n",
    "\n",
    "# Compare cosine similarity with all words\n",
    "print(\"Cosine similarities:\")\n",
    "for w in words:\n",
    "    similarity = F.cosine_similarity(analogy_vector.unsqueeze(0), embeddings[w].unsqueeze(0))\n",
    "    print(f\"{w:>6}: {similarity.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
